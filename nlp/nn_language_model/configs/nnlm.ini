[nnlm]
;数据文件路径
train_data=data/test/seg_words
;隐层size
hidden_size=200
;层数
num_layers=2
;词典大小
vocab_size=1000
;学习率
learning_rate=1.0
;mini batch的每个batch大小
train_batch_size=20
;迭代次数
train_num_step=35
;测试的batch大小（测试数据是一行)
eval_batch_size=1
；每次读取的序列长度
eval_batch_size=1
；使用训练的数据的轮数
num_epoch=2
;节点不被dropout掉的概率
keep_prob=0.5
；梯度膨胀的截断阈值
max_grad_norm=5
